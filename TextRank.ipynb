{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e63ebdb-611e-4837-98e9-9724132db4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2bb23625-ca00-4ddc-b29a-4c3fa710217b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dataset(file):\n",
    "    result = []\n",
    "    with open(file, \"r\", encoding=\"utf8\") as r:\n",
    "        for line in r:\n",
    "            result.append(json.loads(line))\n",
    "    result.sort(key=lambda x: x[\"date\"])\n",
    "    return result\n",
    "\n",
    "\n",
    "train_obj = read_dataset('.\\gazeta_dataset\\gazeta_train.jsonl')\n",
    "test_obj = read_dataset('.\\gazeta_dataset\\gazeta_test.jsonl')\n",
    "val_obj =  read_dataset('.\\gazeta_dataset\\gazeta_val.jsonl')\n",
    "full_obj = train_obj + test_obj + val_obj # для TextRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2ffcd65b-288b-4993-92f5-534faf645590",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(li_word):\n",
    "    global normalized_freq\n",
    "    normalized_freq = []\n",
    "    for dictionary in li_word:\n",
    "        max_frequency = max(dictionary.values())\n",
    "        for word in dictionary.keys():\n",
    "            dictionary[word] = dictionary[word]/max_frequency\n",
    "        normalized_freq.append(dictionary)\n",
    "    return normalized_freq\n",
    "\n",
    "def calc_word_freq(article_word):\n",
    "    calc_word_freq = {}\n",
    "    li_word = []\n",
    "    for sentence in article_word:\n",
    "        for word in word_tokenize(sentence):\n",
    "            if word not in calc_word_freq.keys():\n",
    "                calc_word_freq[word] = 1\n",
    "            else:\n",
    "                calc_word_freq[word] += 1\n",
    "        li_word.append(calc_word_freq)\n",
    "        calc_word_freq = {}\n",
    "    normalize(li_word)\n",
    "    return normalized_freq\n",
    "\n",
    "def sentence_score(sentences):\n",
    "    global sentence_score_list\n",
    "    sentence_score = {}\n",
    "    sentence_score_list = []\n",
    "    for list_, dictionary in zip(sentences, normalized_freq):\n",
    "        for sent in list_:\n",
    "            for word in word_tokenize(sent):\n",
    "                if word in dictionary.keys():\n",
    "                    if sent not in sentence_score.keys():\n",
    "                        sentence_score[sent] = dictionary[word]\n",
    "                    else:\n",
    "                        sentence_score[sent] += dictionary[word]\n",
    "        sentence_score_list.append(sentence_score)\n",
    "        sentence_score = {}\n",
    "    return sentence_score_list\n",
    "\n",
    "# Function to tokenize the sentence\n",
    "def tokenize(article_sent):\n",
    "    sentence_list = []\n",
    "    sent_token = []\n",
    "    for sent in article_sent:\n",
    "        token = tokenizeize(sent)\n",
    "        for sentence in token:\n",
    "            sub_token = ''.join(word for word in sentence if word not in punctuation)\n",
    "            sub_token = re.sub(' +', ' ', sub_token)\n",
    "            sent_token.append(sub_token)\n",
    "        sentence_list.append(sent_token)\n",
    "        sent_token = []\n",
    "    sentence_score(sentence_list)\n",
    "    return sentence_score_list\n",
    "\n",
    "def summary(sentence_score):\n",
    "    summary_list = []\n",
    "    for summ in sentence_score:\n",
    "        select_length = int(len(summ)*0.25)\n",
    "        summary_ = nlargest(select_length, summ, key = summ.get)\n",
    "        summary_list.append(\".\".join(summary_))\n",
    "    return summary_list\n",
    "\n",
    "\n",
    "def to_seris(art):\n",
    "    global dataframe\n",
    "    data_dict = {'article' : [art]}\n",
    "    dataframe = pd.DataFrame(data_dict)['article']\n",
    "    return dataframe\n",
    "\n",
    "#Основная функция\n",
    "def article_summarize(artefact):\n",
    "    if type(artefact) != pd.Series:\n",
    "        artefact = to_seris(artefact)\n",
    "    df = preprocessing(artefact)\n",
    "    word_normalization = calc_word_freq(df)\n",
    "    sentence_score = tokenize(article_sent)\n",
    "    summarized_article = summary(sentence_score)\n",
    "    return summarized_article\n",
    "\n",
    "# article_seried = make_series(article)\n",
    "# article_summarize(article_seried)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd0e265-a233-4ffd-9fbf-5af2365567c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from summa.summarizer import summarize\n",
    "#библиотечный TextRank, для сравнения\n",
    "def predict_text_rank(text, summary, summary_part=0.1):\n",
    "    return summarize(text, ratio=summary_part, language='russian').replace(\"\\n\", \" \")\n",
    "\n",
    "calc_method_score(test_obj, predict_text_rank)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
