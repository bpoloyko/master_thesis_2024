{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "48b71ee7-4418-4a41-9765-40e950eee761",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import string\n",
    "from string import punctuation\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('russian'))\n",
    "punctuation = punctuation + '\\n' + '—' + '“' + ',' + '”' + '‘' + '-' + '’'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5aec1f00-5c2d-44be-9a0b-803ed5b24092",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanhtml(raw_html):\n",
    "    cleanr = re.compile('<.*?>')\n",
    "    cleantext = re.sub(cleanr, '', raw_html)\n",
    "    return cleantext\n",
    "    \n",
    "def preprocessing(article):\n",
    "    global article_sent\n",
    "    article = article.str.lower()\n",
    "    article = article.apply(lambda x: cleanhtml(x))\n",
    "    article = article.apply(lambda x: re.sub('\\S+@\\S+','', x))\n",
    "    article = article.apply(lambda x: re.sub(\"((http\\://|https\\://|ftp\\://)|(www.))+(([a-zA-Z0-9\\.-]+\\.[a-zA-Z]{2,4})|([0-9]{1,3}\\.[0-9]{1,3}\\.[0-9]{1,3}\\.[0-9]{1,3}))(/[a-zA-Z0-9%:/-_\\?\\.'~]*)?\",'', x))\n",
    "    article = article.apply(lambda x: x.replace(\"\\xa0\", \" \"))\n",
    "    article = article.apply(lambda x: x.replace(\"'s\", ''))\n",
    "    article = article.apply(lambda x: x.replace('’s', ''))\n",
    "    article = article.apply(lambda x: x.replace(\"\\'s\", ''))\n",
    "    article = article.apply(lambda x: x.replace(\"\\’s\", ''))\n",
    "    article = article.apply(lambda x: re.sub(' +', ' ',x))\n",
    "    article_sent = article.copy()\n",
    "    article = article.apply(lambda x: ''.join(word for word in x if word not in punctuation))\n",
    "    article = article.apply(lambda x: re.sub(' +', ' ',x))\n",
    "    article = article.apply(lambda x: ' '.join(word for word in x.split() if word not in stop_words))\n",
    "    \n",
    "    return article\n",
    "\n",
    "def normalize(li_word):\n",
    "    global normalized_freq\n",
    "    normalized_freq = []\n",
    "    for dictionary in li_word:\n",
    "        max_frequency = max(dictionary.values())\n",
    "        for word in dictionary.keys():\n",
    "            dictionary[word] = dictionary[word]/max_frequency\n",
    "        normalized_freq.append(dictionary)\n",
    "        \n",
    "    return normalized_freq\n",
    "\n",
    "def word_frequency(article_word):\n",
    "    word_frequency = {}\n",
    "    li_word = []\n",
    "    for sentence in article_word:\n",
    "        for word in word_tokenize(sentence):\n",
    "            if word not in word_frequency.keys():\n",
    "                word_frequency[word] = 1\n",
    "            else:\n",
    "                word_frequency[word] += 1\n",
    "        li_word.append(word_frequency)\n",
    "        word_frequency = {}\n",
    "    normalize(li_word)\n",
    "    \n",
    "    return normalized_freq\n",
    "\n",
    "def sentence_score(li):\n",
    "    global sentence_score_list\n",
    "    sentence_score = {}\n",
    "    sentence_score_list = []\n",
    "    for list_, dictionary in zip(li, normalized_freq):\n",
    "        for sent in list_:\n",
    "            for word in word_tokenize(sent):\n",
    "                if word in dictionary.keys():\n",
    "                    if sent not in sentence_score.keys():\n",
    "                        sentence_score[sent] = dictionary[word]\n",
    "                    else:\n",
    "                        sentence_score[sent] += dictionary[word]\n",
    "        sentence_score_list.append(sentence_score)\n",
    "        sentence_score = {}\n",
    "        \n",
    "    return sentence_score_list\n",
    "\n",
    "def sent_token(article_sent):\n",
    "    sentence_list = []\n",
    "    sent_token = []\n",
    "    for sent in article_sent:\n",
    "        token = sent_tokenize(sent)\n",
    "        for sentence in token:\n",
    "            token_2 = ''.join(word for word in sentence if word not in punctuation)\n",
    "            token_2 = re.sub(' +', ' ',token_2)\n",
    "            sent_token.append(token_2)\n",
    "        sentence_list.append(sent_token)\n",
    "        sent_token = []\n",
    "    sentence_score(sentence_list)\n",
    "    \n",
    "    return sentence_score_list\n",
    "\n",
    "def summary(sentence_score):\n",
    "    summary_list = []\n",
    "    for summ in sentence_score:\n",
    "        select_length = int(len(summ)*0.25)\n",
    "        summary_ = nlargest(select_length, summ, key = summ.get)\n",
    "        summary_list.append(\".\".join(summary_))\n",
    "        \n",
    "    return summary_list\n",
    "\n",
    "def to_series(art):\n",
    "    global dataframe\n",
    "    data_dict = {'article' : [art]}\n",
    "    dataframe = pd.DataFrame(data_dict)['article']\n",
    "    \n",
    "    return dataframe\n",
    "\n",
    "def article_summarize(artefact):\n",
    "    \n",
    "    if type(artefact) != pd.Series:\n",
    "        artefact = to_series(artefact)\n",
    "    df = preprocessing(artefact)\n",
    "    word_normalization = word_frequency(df)\n",
    "    sentence_score = sent_token(article_sent)\n",
    "    summarized_article = summary(sentence_score)\n",
    "    return summarized_article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1d163db7-fe55-4a59-8044-e85f1fcfe441",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['в смысловой цельности текста отражаются те связи и зависимости которые имеются в самой действительности общественные события явления природы человек его внешний облик и внутренний мир предметы неживой природы и т']\n"
     ]
    }
   ],
   "source": [
    "summaries = article_summarize(article)\n",
    "print(summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4fd0e265-a233-4ffd-9fbf-5af2365567c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from summa.summarizer import summarize\n",
    "#библиотечный TextRank, для сравнения\n",
    "def predict_text_rank(text, summary, summary_part=0.1):\n",
    "    return summarize(text, ratio=summary_part, language='russian').replace(\"\\n\", \" \")\n",
    "\n",
    "#calc_method_score(test_obj, predict_text_rank)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
