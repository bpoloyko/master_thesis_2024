{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "28251d1d-3a93-4d1b-a15c-fe5263f8a5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset, load_metric\n",
    "import numpy as np\n",
    "import nltk\n",
    "from transformers import AutoTokenizer, T5TokenizerFast, Seq2SeqTrainingArguments, Seq2SeqTrainer, AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq\n",
    "import torch\n",
    "import pandas as pd\n",
    "from transformers.optimization import Adafactor, AdafactorSchedule"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae59a570-9585-48df-afea-97da885aa137",
   "metadata": {},
   "source": [
    "# –í–∞—Ä–∏–∞–Ω—Ç 1. –°–∞–º–æ—Å—Ç–æ—è—Ç–µ–ª—å–Ω—ã–π fine-tune –º–æ–¥–µ–ª–∏"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ab95f67-b364-4bc3-81a8-3285625a3d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "gazeta = load_dataset(\"IlyaGusev/gazeta\", revision=\"v2.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "05223e4b-8c1b-4233-9651-69424e240cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –∫–æ–¥–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –º–æ–¥–µ–ª—å—é\n",
    "def encode_data(input_sequences):\n",
    "  task_prefix = \"summarize: \"\n",
    "  if type(input_sequences) != list: \n",
    "    input_sequences = [input_sequences]\n",
    "  encoded = tokenizer(\n",
    "    [task_prefix + sequence for sequence in input_sequences],\n",
    "    padding=\"longest\",\n",
    "    max_length=max_input,\n",
    "    truncation=True,\n",
    "    return_tensors=\"pt\",)\n",
    "  return encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "101936f8-e325-4ee6-99bd-56abf03fb051",
   "metadata": {},
   "outputs": [],
   "source": [
    "#—Ç–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ–º –¥–∞—Ç–∞—Å–µ—Ç\n",
    "def encode_dataset(dataset, max_output = 64):\n",
    "    INPUT_IDS = []\n",
    "    ATTENTION_MASK = []\n",
    "    LABELS = []\n",
    "    for i in range(len(dataset)):\n",
    "        encoded_row = encode_data(dataset[i]['text'])\n",
    "        input_ids, attention_mask = encoded_row.input_ids, encoded_row.attention_mask\n",
    "        target_encoding = tokenizer(dataset[i]['summary'], padding=\"longest\", max_length=max_output, truncation=True)\n",
    "        labels = target_encoding.input_ids\n",
    "        labels = torch.tensor(labels)\n",
    "        labels[labels == tokenizer.pad_token_id] = -100\n",
    "        INPUT_IDS.append(input_ids)\n",
    "        ATTENTION_MASK.append(ATTENTION_MASK)\n",
    "        LABELS.append(labels)\n",
    "    data = Dataset.from_pandas(pd.DataFrame({'input_ids': list(np.array(INPUT_IDS)), 'attention_mask': list(np.array(ATTENTION_MASK)), 'labels': list(np.array(LABELS))}))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "23fe0004-74aa-43b4-bc90-5c97d6aa04a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60964"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(gazeta['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28fbfd3d-bfc4-4a21-b514-8426ef0ae4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_gazeta = encode_dataset(gazeta['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2df7d1e5-dfb1-4f2a-9a5c-a9030aeaf571",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a87f7d71f6e048b381aaad22731f83e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading metadata:   0%|          | 0.00/1.04k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35faf5152bba4259abcf133bf92a302b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcb138291ecd49198d15d3ca6e8e2a60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/49.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88d799147f554d87af7a0b6d5882f53b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/49.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4d1ed23fcc2423c9ddc8d5496e42ba7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/49.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3d525f8635e4553b2f0613fc518ee01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/49.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bca423cc92024008a2fd57d4a561c5d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/10.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8c55d80f0e54683b119b9d37ec9102a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32fc6eecf6c444aa9475823cbcf125db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/452275 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc56b96f2c634a539747595e8b36f31b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/23804 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç\n",
    "urukhan = load_dataset('UrukHan/t5-russian-summarization' )\n",
    "urukhan_train = urukhan['train']\n",
    "urukhan_test = urukhan['test'].train_test_split(0.02)['test'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "53ad3ca3-5915-4e73-a69d-18f2788c317c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Boris\\AppData\\Local\\Temp\\ipykernel_12644\\2179309921.py:1: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ü§ó Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = load_metric(\"rouge\")\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Boris\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric = load_metric(\"rouge\")\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "38102403-3844-432e-bfb2-7bfa8bdae978",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'UrukHan/t5-russian-summarization' # –ù–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ –∏–∑ HuggingFace Hub\n",
    "max_input = 1024  # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–Ω–∞ –≤—Ö–æ–¥–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ (–≤ —Ç–æ–∫–µ–Ω–∞—Ö)\n",
    "max_output  = 64  # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–Ω–∞ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –∫—Ä–∞—Ç–∫–æ–≥–æ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—è (–≤ —Ç–æ–∫–µ–Ω–∞—Ö)\n",
    "batch_size = 8 \n",
    "output_dir = 'tmp_trainer'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3e164866-c195-4299-bfc0-346874ae26ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = T5TokenizerFast.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9db44c4b-75e8-48aa-9015-0f95ad06dad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.max_length = max_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ec2baa99-48bd-482e-bd97-37fbe2a3f603",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = gazeta['train']\n",
    "test = gazeta['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "95c4ce28-122e-4806-a419-83500a7885d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "  predictions, labels = eval_pred\n",
    "  decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "  labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "  decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "  decoded_preds = [\"\\n\".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds]\n",
    "  decoded_labels = [\"\\n\".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels]\n",
    "  result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "  result = {key: value.mid.fmeasure * 100 for key, value in result.items()}\n",
    "  prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n",
    "  result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "  \n",
    "  return {k: round(v, 4) for k, v in result.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51624ab-20e4-4606-8ff6-75460abc9cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –í–ù–ò–ú–ê–ù–ò–ï! –ù–£–ñ–ù–ê CUDA. –ú–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å Google Collabs\n",
    "# pip install transformers[torch]\n",
    "\n",
    "# test = urukhan_test\n",
    "# train = urukhan_train\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "  output_dir = output_dir,\n",
    "  evaluation_strategy='steps',\n",
    "  #learning_rate=2e-5,\n",
    "  eval_steps=5000,\n",
    "  save_steps=5000,\n",
    "  num_train_epochs=1,\n",
    "  predict_with_generate=True,\n",
    "  per_device_train_batch_size=batch_size,\n",
    "  per_device_eval_batch_size=batch_size,\n",
    "  fp16=True,\n",
    "  save_total_limit=2,\n",
    "  #generation_max_length=256,\n",
    "  #generation_num_beams=4,\n",
    "  weight_decay=0.005,\n",
    "  #logging_dir='logs',\n",
    ")\n",
    "\n",
    "# –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä\n",
    "optimizer = Adafactor(\n",
    "    model.parameters(),\n",
    "    lr=1e-5,\n",
    "    eps=(1e-30, 1e-3),\n",
    "    clip_threshold=1.0,\n",
    "    decay_rate=-0.8,\n",
    "    beta1=None,\n",
    "    weight_decay=0.0,\n",
    "    relative_step=False,\n",
    "    scale_parameter=False,\n",
    "    warmup_init=False,\n",
    ")\n",
    "lr_scheduler = AdafactorSchedule(optimizer)\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "  model=model,\n",
    "  args=training_args,\n",
    "  train_dataset = train,\n",
    "  eval_dataset = test,\n",
    "  optimizers = (optimizer, lr_scheduler),\n",
    "  tokenizer = tokenizer,\n",
    "  compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af27a3b9-1d11-440e-af56-d0d4913be9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47be8742-64a7-4db4-a7f0-984196ced45c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a2301d2e-c706-4e20-9a83-36a3885b94c3",
   "metadata": {},
   "source": [
    "# –í–∞—Ä–∏–∞–Ω—Ç 2. –ü—Ä–æ—Å—Ç–æ –∏—Å–ø–æ–ª—å –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "307ea349-4d90-43a5-8078-7b4b013e8363",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b7211c3f1d54545b6656282a2c5b6f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m model \u001b[38;5;241m=\u001b[39m T5ForConditionalGeneration\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRussianNLP/FRED-T5-Summarizer\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      5\u001b[0m device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 6\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m input_text\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<LM> –°–æ–∫—Ä–∞—Ç–∏ —Ç–µ–∫—Å—Ç.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m –¢–µ–∫—Å—Ç (–æ—Ç –ª–∞—Ç. textus ‚Äî —Ç–∫–∞–Ω—å; —Å–ø–ª–µ—Ç–µ–Ω–∏–µ, —Å–æ—á–µ—Ç–∞–Ω–∏–µ) ‚Äî –∑–∞—Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –Ω–∞ –∫–∞–∫–æ–º-–ª–∏–±–æ –º–∞—Ç–µ—Ä–∏–∞–ª—å–Ω–æ–º –Ω–æ—Å–∏—Ç–µ–ª–µ —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∞—è –º—ã—Å–ª—å; –≤ –æ–±—â–µ–º –ø–ª–∞–Ω–µ —Å–≤—è–∑–Ω–∞—è –∏ –ø–æ–ª–Ω–∞—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Å–∏–º–≤–æ–ª–æ–≤. –°—É—â–µ—Å—Ç–≤—É—é—Ç –¥–≤–µ –æ—Å–Ω–æ–≤–Ω—ã–µ —Ç—Ä–∞–∫—Ç–æ–≤–∫–∏ –ø–æ–Ω—è—Ç–∏—è ¬´—Ç–µ–∫—Å—Ç¬ª: –∏–º–º–∞–Ω–µ–Ω—Ç–Ω–∞—è (—Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è, —Ñ–∏–ª–æ—Å–æ—Ñ—Å–∫–∏ –Ω–∞–≥—Ä—É–∂–µ–Ω–Ω–∞—è) –∏ —Ä–µ–ø—Ä–µ–∑–µ–Ω—Ç–∞—Ç–∏–≤–Ω–∞—è (–±–æ–ª–µ–µ —á–∞—Å—Ç–Ω–∞—è). –ò–º–º–∞–Ω–µ–Ω—Ç–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –ø–æ–¥—Ä–∞–∑—É–º–µ–≤–∞–µ—Ç –æ—Ç–Ω–æ—à–µ–Ω–∏–µ –∫ —Ç–µ–∫—Å—Ç—É –∫–∞–∫ –∫ –∞–≤—Ç–æ–Ω–æ–º–Ω–æ–π —Ä–µ–∞–ª—å–Ω–æ—Å—Ç–∏, –Ω–∞—Ü–µ–ª–µ–Ω–Ω–æ—Å—Ç—å –Ω–∞ –≤—ã—è–≤–ª–µ–Ω–∏–µ –µ–≥–æ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–π —Å—Ç—Ä—É–∫—Ç—É—Ä—ã. –†–µ–ø—Ä–µ–∑–µ–Ω—Ç–∞—Ç–∏–≤–Ω—ã–π ‚Äî —Ä–∞—Å—Å–º–æ—Ç—Ä–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∫–∞–∫ –æ—Å–æ–±–æ–π —Ñ–æ—Ä–º—ã –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –≤–Ω–µ—à–Ω–µ–π —Ç–µ–∫—Å—Ç—É –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      9\u001b[0m input_ids\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mtensor([tokenizer\u001b[38;5;241m.\u001b[39mencode(input_text)])\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[1;32mC:\\Python311\\Lib\\site-packages\\transformers\\modeling_utils.py:2692\u001b[0m, in \u001b[0;36mPreTrainedModel.to\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2687\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_present_in_args:\n\u001b[0;32m   2688\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   2689\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou cannot cast a GPTQ model in a new `dtype`. Make sure to load the model using `from_pretrained` using the desired\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2690\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `dtype` by passing the correct `torch_dtype` argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2691\u001b[0m         )\n\u001b[1;32m-> 2692\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1173\u001b[0m, in \u001b[0;36mModule.to\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1170\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1171\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m-> 1173\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:779\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    777\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[0;32m    778\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 779\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    781\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    782\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    783\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    784\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    789\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    790\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:804\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    800\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[0;32m    801\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[0;32m    802\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[0;32m    803\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m--> 804\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    805\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[0;32m    807\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1159\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m   1152\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[0;32m   1153\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[0;32m   1154\u001b[0m             device,\n\u001b[0;32m   1155\u001b[0m             dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1156\u001b[0m             non_blocking,\n\u001b[0;32m   1157\u001b[0m             memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format,\n\u001b[0;32m   1158\u001b[0m         )\n\u001b[1;32m-> 1159\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1161\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1162\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1163\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1164\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1165\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot copy out of meta tensor; no data!\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32mC:\\Python311\\Lib\\site-packages\\torch\\cuda\\__init__.py:284\u001b[0m, in \u001b[0;36m_lazy_init\u001b[1;34m()\u001b[0m\n\u001b[0;32m    279\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    280\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    281\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultiprocessing, you must use the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspawn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m start method\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    282\u001b[0m     )\n\u001b[0;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(torch\u001b[38;5;241m.\u001b[39m_C, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_cuda_getDeviceCount\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 284\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTorch not compiled with CUDA enabled\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    285\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _cudart \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    286\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[0;32m    287\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    288\u001b[0m     )\n",
      "\u001b[1;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, T5ForConditionalGeneration \n",
    "tokenizer = GPT2Tokenizer.from_pretrained('RussianNLP/FRED-T5-Summarizer',eos_token='</s>')\n",
    "model = T5ForConditionalGeneration.from_pretrained('RussianNLP/FRED-T5-Summarizer')\n",
    "device='cuda'\n",
    "model.to(device)\n",
    "\n",
    "input_text='<LM> –°–æ–∫—Ä–∞—Ç–∏ —Ç–µ–∫—Å—Ç.\\n –¢–µ–∫—Å—Ç (–æ—Ç –ª–∞—Ç. textus ‚Äî —Ç–∫–∞–Ω—å; —Å–ø–ª–µ—Ç–µ–Ω–∏–µ, —Å–æ—á–µ—Ç–∞–Ω–∏–µ) ‚Äî –∑–∞—Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –Ω–∞ –∫–∞–∫–æ–º-–ª–∏–±–æ –º–∞—Ç–µ—Ä–∏–∞–ª—å–Ω–æ–º –Ω–æ—Å–∏—Ç–µ–ª–µ —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∞—è –º—ã—Å–ª—å; –≤ –æ–±—â–µ–º –ø–ª–∞–Ω–µ —Å–≤—è–∑–Ω–∞—è –∏ –ø–æ–ª–Ω–∞—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Å–∏–º–≤–æ–ª–æ–≤. –°—É—â–µ—Å—Ç–≤—É—é—Ç –¥–≤–µ –æ—Å–Ω–æ–≤–Ω—ã–µ —Ç—Ä–∞–∫—Ç–æ–≤–∫–∏ –ø–æ–Ω—è—Ç–∏—è ¬´—Ç–µ–∫—Å—Ç¬ª: –∏–º–º–∞–Ω–µ–Ω—Ç–Ω–∞—è (—Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è, —Ñ–∏–ª–æ—Å–æ—Ñ—Å–∫–∏ –Ω–∞–≥—Ä—É–∂–µ–Ω–Ω–∞—è) –∏ —Ä–µ–ø—Ä–µ–∑–µ–Ω—Ç–∞—Ç–∏–≤–Ω–∞—è (–±–æ–ª–µ–µ —á–∞—Å—Ç–Ω–∞—è). –ò–º–º–∞–Ω–µ–Ω—Ç–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –ø–æ–¥—Ä–∞–∑—É–º–µ–≤–∞–µ—Ç –æ—Ç–Ω–æ—à–µ–Ω–∏–µ –∫ —Ç–µ–∫—Å—Ç—É –∫–∞–∫ –∫ –∞–≤—Ç–æ–Ω–æ–º–Ω–æ–π —Ä–µ–∞–ª—å–Ω–æ—Å—Ç–∏, –Ω–∞—Ü–µ–ª–µ–Ω–Ω–æ—Å—Ç—å –Ω–∞ –≤—ã—è–≤–ª–µ–Ω–∏–µ –µ–≥–æ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–π —Å—Ç—Ä—É–∫—Ç—É—Ä—ã. –†–µ–ø—Ä–µ–∑–µ–Ω—Ç–∞—Ç–∏–≤–Ω—ã–π ‚Äî —Ä–∞—Å—Å–º–æ—Ç—Ä–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∫–∞–∫ –æ—Å–æ–±–æ–π —Ñ–æ—Ä–º—ã –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –≤–Ω–µ—à–Ω–µ–π —Ç–µ–∫—Å—Ç—É –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏.'\n",
    "input_ids=torch.tensor([tokenizer.encode(input_text)]).to(device)\n",
    "outputs=model.generate(input_ids,eos_token_id=tokenizer.eos_token_id,\n",
    "                    num_beams=5,\n",
    "                    min_new_tokens=17,\n",
    "                    max_new_tokens=200,\n",
    "                    do_sample=True,\n",
    "                    no_repeat_ngram_size=4,\n",
    "                    top_p=0.9)\n",
    "print(tokenizer.decode(outputs[0][1:]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
