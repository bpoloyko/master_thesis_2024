{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28251d1d-3a93-4d1b-a15c-fe5263f8a5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset, load_metric\n",
    "import numpy as np\n",
    "import nltk\n",
    "from transformers import AutoTokenizer, T5TokenizerFast, Seq2SeqTrainingArguments, Seq2SeqTrainer, AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq\n",
    "import torch\n",
    "from transformers.optimization import Adafactor, AdafactorSchedule"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae59a570-9585-48df-afea-97da885aa137",
   "metadata": {},
   "source": [
    "# –í–∞—Ä–∏–∞–Ω—Ç 1. –°–∞–º–æ—Å—Ç–æ—è—Ç–µ–ª—å–Ω—ã–π fine-tune –º–æ–¥–µ–ª–∏"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ab95f67-b364-4bc3-81a8-3285625a3d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "gazeta = load_dataset(\"IlyaGusev/gazeta\", revision=\"v2.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5f7d5f81-4c06-44c0-a383-3e99601dd033",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'summary', 'title', 'date', 'url'],\n",
       "    num_rows: 60964\n",
       "})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gazeta['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "05223e4b-8c1b-4233-9651-69424e240cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# –∫–æ–¥–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –º–æ–¥–µ–ª—å—é\n",
    "def encode(input_sequences):\n",
    "  task_prefix = \"summarize: \"\n",
    "  if type(input_sequences) != list: \n",
    "    input_sequences = [input_sequences]\n",
    "  encoded = tokenizer(\n",
    "    [task_prefix + sequence for sequence in input_sequences],\n",
    "    padding=\"longest\",\n",
    "    max_length=max_input,\n",
    "    truncation=True,\n",
    "    return_tensors=\"pt\",)\n",
    "  return encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "101936f8-e325-4ee6-99bd-56abf03fb051",
   "metadata": {},
   "outputs": [],
   "source": [
    "#—Ç–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ–º –¥–∞—Ç–∞—Å–µ—Ç\n",
    "def encode_dataset(dataset, max_output = 64):\n",
    "    INPUT_IDS = []\n",
    "    ATTENTION_MASK = []\n",
    "    LABELS = []\n",
    "    for row in dataset:\n",
    "        encoded_row = encode(row['text'])\n",
    "        input_ids, attention_mask = encoded_row.input_ids, encoded_row.attention_mask\n",
    "        target_encoding = tokenizer(row['summary'], padding=\"longest\", max_length=max_output, truncation=True)\n",
    "        labels = target_encoding.input_ids\n",
    "        labels = torch.tensor(labels)\n",
    "        labels[labels == tokenizer.pad_token_id] = -100\n",
    "        INPUT_IDS.append(input_ids)\n",
    "        ATTENTION_MASK.append(ATTENTION_MASK)\n",
    "        LABELS.append(labels)\n",
    "    data = Dataset.from_pandas(pd.DataFrame({'input_ids': list(np.array(INPUT_IDS)), 'attention_mask': list(np.array(ATTENTION_MASK)), 'labels': list(np.array(LABELS))}))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace323b9-54ee-48f0-896d-ba90e26a73be",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_gazeta = encode_dataset(gazeta['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28fbfd3d-bfc4-4a21-b514-8426ef0ae4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_gazeta[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2df7d1e5-dfb1-4f2a-9a5c-a9030aeaf571",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a87f7d71f6e048b381aaad22731f83e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading metadata:   0%|          | 0.00/1.04k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35faf5152bba4259abcf133bf92a302b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcb138291ecd49198d15d3ca6e8e2a60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/49.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88d799147f554d87af7a0b6d5882f53b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/49.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4d1ed23fcc2423c9ddc8d5496e42ba7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/49.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3d525f8635e4553b2f0613fc518ee01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/49.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bca423cc92024008a2fd57d4a561c5d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/10.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8c55d80f0e54683b119b9d37ec9102a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32fc6eecf6c444aa9475823cbcf125db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/452275 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc56b96f2c634a539747595e8b36f31b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/23804 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç\n",
    "urukhan = load_dataset('UrukHan/t5-russian-summarization' )\n",
    "urukhan_train = urukhan['train']\n",
    "urukhan_test = urukhan['test'].train_test_split(0.02)['test'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "53ad3ca3-5915-4e73-a69d-18f2788c317c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Boris\\AppData\\Local\\Temp\\ipykernel_12644\\2179309921.py:1: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ü§ó Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = load_metric(\"rouge\")\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Boris\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric = load_metric(\"rouge\")\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "38102403-3844-432e-bfb2-7bfa8bdae978",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'UrukHan/t5-russian-summarization' # –ù–∞–∑–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ –∏–∑ HuggingFace Hub\n",
    "max_input = 1024  # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–Ω–∞ –≤—Ö–æ–¥–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ (–≤ —Ç–æ–∫–µ–Ω–∞—Ö)\n",
    "max_output  = 64  # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–Ω–∞ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –∫—Ä–∞—Ç–∫–æ–≥–æ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—è (–≤ —Ç–æ–∫–µ–Ω–∞—Ö)\n",
    "batch_size = 8 \n",
    "output_dir = 'tmp_trainer'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3e164866-c195-4299-bfc0-346874ae26ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = T5TokenizerFast.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9db44c4b-75e8-48aa-9015-0f95ad06dad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.max_length = max_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ec2baa99-48bd-482e-bd97-37fbe2a3f603",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = gazeta['train']\n",
    "test = gazeta['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "95c4ce28-122e-4806-a419-83500a7885d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "  predictions, labels = eval_pred\n",
    "  decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "  labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "  decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "  decoded_preds = [\"\\n\".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds]\n",
    "  decoded_labels = [\"\\n\".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels]\n",
    "  result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "  result = {key: value.mid.fmeasure * 100 for key, value in result.items()}\n",
    "  prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n",
    "  result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "  \n",
    "  return {k: round(v, 4) for k, v in result.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "921a7880-b9f7-482e-b9ce-e7a5d64c7606",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "FP16 Mixed precision training with AMP or APEX (`--fp16`) and FP16 half precision evaluation (`--fp16_full_eval`) can only be used on CUDA or MLU devices or NPU devices or certain XPU devices (with IPEX).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 7\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# –í–ù–ò–ú–ê–ù–ò–ï! –ù–£–ñ–ù–ê CUDA. –ú–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å Google Collabs\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# pip install transformers[torch]\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# test = urukhan_test\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# train = urukhan_train\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m training_args \u001b[38;5;241m=\u001b[39m \u001b[43mSeq2SeqTrainingArguments\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m  \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m  \u001b[49m\u001b[43mevaluation_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msteps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m  \u001b[49m\u001b[38;5;66;43;03m#learning_rate=2e-5,\u001b[39;49;00m\n\u001b[0;32m     11\u001b[0m \u001b[43m  \u001b[49m\u001b[43meval_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m  \u001b[49m\u001b[43msave_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m  \u001b[49m\u001b[43mnum_train_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m  \u001b[49m\u001b[43mpredict_with_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m  \u001b[49m\u001b[43mper_device_train_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m  \u001b[49m\u001b[43mper_device_eval_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m  \u001b[49m\u001b[43mfp16\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m  \u001b[49m\u001b[43msave_total_limit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m  \u001b[49m\u001b[38;5;66;43;03m#generation_max_length=256,\u001b[39;49;00m\n\u001b[0;32m     20\u001b[0m \u001b[43m  \u001b[49m\u001b[38;5;66;43;03m#generation_num_beams=4,\u001b[39;49;00m\n\u001b[0;32m     21\u001b[0m \u001b[43m  \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.005\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[43m  \u001b[49m\u001b[38;5;66;43;03m#logging_dir='logs',\u001b[39;49;00m\n\u001b[0;32m     23\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä\u001b[39;00m\n\u001b[0;32m     26\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m Adafactor(\n\u001b[0;32m     27\u001b[0m     model\u001b[38;5;241m.\u001b[39mparameters(),\n\u001b[0;32m     28\u001b[0m     lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-5\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     36\u001b[0m     warmup_init\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     37\u001b[0m )\n",
      "File \u001b[1;32m<string>:130\u001b[0m, in \u001b[0;36m__init__\u001b[1;34m(self, output_dir, overwrite_output_dir, do_train, do_eval, do_predict, evaluation_strategy, prediction_loss_only, per_device_train_batch_size, per_device_eval_batch_size, per_gpu_train_batch_size, per_gpu_eval_batch_size, gradient_accumulation_steps, eval_accumulation_steps, eval_delay, learning_rate, weight_decay, adam_beta1, adam_beta2, adam_epsilon, max_grad_norm, num_train_epochs, max_steps, lr_scheduler_type, lr_scheduler_kwargs, warmup_ratio, warmup_steps, log_level, log_level_replica, log_on_each_node, logging_dir, logging_strategy, logging_first_step, logging_steps, logging_nan_inf_filter, save_strategy, save_steps, save_total_limit, save_safetensors, save_on_each_node, save_only_model, no_cuda, use_cpu, use_mps_device, seed, data_seed, jit_mode_eval, use_ipex, bf16, fp16, fp16_opt_level, half_precision_backend, bf16_full_eval, fp16_full_eval, tf32, local_rank, ddp_backend, tpu_num_cores, tpu_metrics_debug, debug, dataloader_drop_last, eval_steps, dataloader_num_workers, dataloader_prefetch_factor, past_index, run_name, disable_tqdm, remove_unused_columns, label_names, load_best_model_at_end, metric_for_best_model, greater_is_better, ignore_data_skip, fsdp, fsdp_min_num_params, fsdp_config, fsdp_transformer_layer_cls_to_wrap, accelerator_config, deepspeed, label_smoothing_factor, optim, optim_args, adafactor, group_by_length, length_column_name, report_to, ddp_find_unused_parameters, ddp_bucket_cap_mb, ddp_broadcast_buffers, dataloader_pin_memory, dataloader_persistent_workers, skip_memory_metrics, use_legacy_prediction_loop, push_to_hub, resume_from_checkpoint, hub_model_id, hub_strategy, hub_token, hub_private_repo, hub_always_push, gradient_checkpointing, gradient_checkpointing_kwargs, include_inputs_for_metrics, eval_do_concat_batches, fp16_backend, push_to_hub_model_id, push_to_hub_organization, push_to_hub_token, mp_parameters, auto_find_batch_size, full_determinism, torchdynamo, ray_scope, ddp_timeout, torch_compile, torch_compile_backend, torch_compile_mode, dispatch_batches, split_batches, include_tokens_per_second, include_num_input_tokens_seen, neftune_noise_alpha, optim_target_modules, sortish_sampler, predict_with_generate, generation_max_length, generation_num_beams, generation_config)\u001b[0m\n",
      "File \u001b[1;32mC:\\Python311\\Lib\\site-packages\\transformers\\training_args.py:1612\u001b[0m, in \u001b[0;36mTrainingArguments.__post_init__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1600\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--optim adamw_torch_fused with --fp16 requires PyTorch>2.0\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1602\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   1603\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1604\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m is_torch_available()\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1610\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp16 \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp16_full_eval)\n\u001b[0;32m   1611\u001b[0m ):\n\u001b[1;32m-> 1612\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1613\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFP16 Mixed precision training with AMP or APEX (`--fp16`) and FP16 half precision evaluation\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1614\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m (`--fp16_full_eval`) can only be used on CUDA or MLU devices or NPU devices or certain XPU devices (with IPEX).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1615\u001b[0m     )\n\u001b[0;32m   1617\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   1618\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1619\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m is_torch_available()\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1627\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbf16 \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbf16_full_eval)\n\u001b[0;32m   1628\u001b[0m ):\n\u001b[0;32m   1629\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1630\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBF16 Mixed precision training with AMP (`--bf16`) and BF16 half precision evaluation\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1631\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m (`--bf16_full_eval`) can only be used on CUDA, XPU (with IPEX), NPU, MLU or CPU/TPU/NeuronCore devices.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1632\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: FP16 Mixed precision training with AMP or APEX (`--fp16`) and FP16 half precision evaluation (`--fp16_full_eval`) can only be used on CUDA or MLU devices or NPU devices or certain XPU devices (with IPEX)."
     ]
    }
   ],
   "source": [
    "# –í–ù–ò–ú–ê–ù–ò–ï! –ù–£–ñ–ù–ê CUDA. –ú–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å Google Collabs\n",
    "# pip install transformers[torch]\n",
    "\n",
    "# test = urukhan_test\n",
    "# train = urukhan_train\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "  output_dir = output_dir,\n",
    "  evaluation_strategy='steps',\n",
    "  #learning_rate=2e-5,\n",
    "  eval_steps=5000,\n",
    "  save_steps=5000,\n",
    "  num_train_epochs=1,\n",
    "  predict_with_generate=True,\n",
    "  per_device_train_batch_size=batch_size,\n",
    "  per_device_eval_batch_size=batch_size,\n",
    "  fp16=True,\n",
    "  save_total_limit=2,\n",
    "  #generation_max_length=256,\n",
    "  #generation_num_beams=4,\n",
    "  weight_decay=0.005,\n",
    "  #logging_dir='logs',\n",
    ")\n",
    "\n",
    "# –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä\n",
    "optimizer = Adafactor(\n",
    "    model.parameters(),\n",
    "    lr=1e-5,\n",
    "    eps=(1e-30, 1e-3),\n",
    "    clip_threshold=1.0,\n",
    "    decay_rate=-0.8,\n",
    "    beta1=None,\n",
    "    weight_decay=0.0,\n",
    "    relative_step=False,\n",
    "    scale_parameter=False,\n",
    "    warmup_init=False,\n",
    ")\n",
    "lr_scheduler = AdafactorSchedule(optimizer)\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "  model=model,\n",
    "  args=training_args,\n",
    "  train_dataset = train,\n",
    "  eval_dataset = test,\n",
    "  optimizers = (optimizer, lr_scheduler),\n",
    "  tokenizer = tokenizer,\n",
    "  compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51624ab-20e4-4606-8ff6-75460abc9cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "61102b41-7ef8-4341-a43f-5012ec777ce0",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mC:\\Python311\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:267\u001b[0m, in \u001b[0;36mBatchEncoding.__getattr__\u001b[1;34m(self, item)\u001b[0m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 267\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n",
      "\u001b[1;31mKeyError\u001b[0m: 'shape'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m input_sequences \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m–¢–µ–∫—Å—Ç (–æ—Ç –ª–∞—Ç. textus ‚Äî —Ç–∫–∞–Ω—å; —Å–ø–ª–µ—Ç–µ–Ω–∏–µ, —Å–æ—á–µ—Ç–∞–Ω–∏–µ) ‚Äî –∑–∞—Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –Ω–∞ –∫–∞–∫–æ–º-–ª–∏–±–æ –º–∞—Ç–µ—Ä–∏–∞–ª—å–Ω–æ–º –Ω–æ—Å–∏—Ç–µ–ª–µ —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∞—è –º—ã—Å–ª—å; –≤ –æ–±—â–µ–º –ø–ª–∞–Ω–µ —Å–≤—è–∑–Ω–∞—è –∏ –ø–æ–ª–Ω–∞—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Å–∏–º–≤–æ–ª–æ–≤.\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m–°—É—â–µ—Å—Ç–≤—É—é—Ç –¥–≤–µ –æ—Å–Ω–æ–≤–Ω—ã–µ —Ç—Ä–∞–∫—Ç–æ–≤–∫–∏ –ø–æ–Ω—è—Ç–∏—è ¬´—Ç–µ–∫—Å—Ç¬ª: –∏–º–º–∞–Ω–µ–Ω—Ç–Ω–∞—è (—Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è, —Ñ–∏–ª–æ—Å–æ—Ñ—Å–∫–∏ –Ω–∞–≥—Ä—É–∂–µ–Ω–Ω–∞—è) –∏ —Ä–µ–ø—Ä–µ–∑–µ–Ω—Ç–∞—Ç–∏–≤–Ω–∞—è (–±–æ–ª–µ–µ —á–∞—Å—Ç–Ω–∞—è). –ò–º–º–∞–Ω–µ–Ω—Ç–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –ø–æ–¥—Ä–∞–∑—É–º–µ–≤–∞–µ—Ç –æ—Ç–Ω–æ—à–µ–Ω–∏–µ –∫ —Ç–µ–∫—Å—Ç—É –∫–∞–∫ –∫ –∞–≤—Ç–æ–Ω–æ–º–Ω–æ–π —Ä–µ–∞–ª—å–Ω–æ—Å—Ç–∏, –Ω–∞—Ü–µ–ª–µ–Ω–Ω–æ—Å—Ç—å –Ω–∞ –≤—ã—è–≤–ª–µ–Ω–∏–µ –µ–≥–æ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–π —Å—Ç—Ä—É–∫—Ç—É—Ä—ã.\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m–†–µ–ø—Ä–µ–∑–µ–Ω—Ç–∞—Ç–∏–≤–Ω—ã–π ‚Äî —Ä–∞—Å—Å–º–æ—Ç—Ä–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∫–∞–∫ –æ—Å–æ–±–æ–π —Ñ–æ—Ä–º—ã –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –≤–Ω–µ—à–Ω–µ–π —Ç–µ–∫—Å—Ç—É –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏.\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m----> 3\u001b[0m predicts \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_sequences\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \n\u001b[0;32m      5\u001b[0m decoded \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mbatch_decode(predicts, skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mC:\\Python311\\Lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Python311\\Lib\\site-packages\\transformers\\generation\\utils.py:1415\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[1;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[0;32m   1407\u001b[0m \u001b[38;5;66;03m# 3. Define model inputs\u001b[39;00m\n\u001b[0;32m   1408\u001b[0m \u001b[38;5;66;03m# inputs_tensor has to be defined\u001b[39;00m\n\u001b[0;32m   1409\u001b[0m \u001b[38;5;66;03m# model_input_name is defined if model-specific keyword input is passed\u001b[39;00m\n\u001b[0;32m   1410\u001b[0m \u001b[38;5;66;03m# otherwise model_input_name is None\u001b[39;00m\n\u001b[0;32m   1411\u001b[0m \u001b[38;5;66;03m# all model-specific keyword inputs are removed from `model_kwargs`\u001b[39;00m\n\u001b[0;32m   1412\u001b[0m inputs_tensor, model_input_name, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_model_inputs(\n\u001b[0;32m   1413\u001b[0m     inputs, generation_config\u001b[38;5;241m.\u001b[39mbos_token_id, model_kwargs\n\u001b[0;32m   1414\u001b[0m )\n\u001b[1;32m-> 1415\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[43minputs_tensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1417\u001b[0m \u001b[38;5;66;03m# 4. Define other model kwargs\u001b[39;00m\n\u001b[0;32m   1418\u001b[0m model_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_attentions\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m generation_config\u001b[38;5;241m.\u001b[39moutput_attentions\n",
      "File \u001b[1;32mC:\\Python311\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:269\u001b[0m, in \u001b[0;36mBatchEncoding.__getattr__\u001b[1;34m(self, item)\u001b[0m\n\u001b[0;32m    267\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata[item]\n\u001b[0;32m    268\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[1;32m--> 269\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "input_sequences = ['–¢–µ–∫—Å—Ç (–æ—Ç –ª–∞—Ç. textus ‚Äî —Ç–∫–∞–Ω—å; —Å–ø–ª–µ—Ç–µ–Ω–∏–µ, —Å–æ—á–µ—Ç–∞–Ω–∏–µ) ‚Äî –∑–∞—Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –Ω–∞ –∫–∞–∫–æ–º-–ª–∏–±–æ –º–∞—Ç–µ—Ä–∏–∞–ª—å–Ω–æ–º –Ω–æ—Å–∏—Ç–µ–ª–µ —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∞—è –º—ã—Å–ª—å; –≤ –æ–±—â–µ–º –ø–ª–∞–Ω–µ —Å–≤—è–∑–Ω–∞—è –∏ –ø–æ–ª–Ω–∞—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Å–∏–º–≤–æ–ª–æ–≤.', '–°—É—â–µ—Å—Ç–≤—É—é—Ç –¥–≤–µ –æ—Å–Ω–æ–≤–Ω—ã–µ —Ç—Ä–∞–∫—Ç–æ–≤–∫–∏ –ø–æ–Ω—è—Ç–∏—è ¬´—Ç–µ–∫—Å—Ç¬ª: –∏–º–º–∞–Ω–µ–Ω—Ç–Ω–∞—è (—Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è, —Ñ–∏–ª–æ—Å–æ—Ñ—Å–∫–∏ –Ω–∞–≥—Ä—É–∂–µ–Ω–Ω–∞—è) –∏ —Ä–µ–ø—Ä–µ–∑–µ–Ω—Ç–∞—Ç–∏–≤–Ω–∞—è (–±–æ–ª–µ–µ —á–∞—Å—Ç–Ω–∞—è). –ò–º–º–∞–Ω–µ–Ω—Ç–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –ø–æ–¥—Ä–∞–∑—É–º–µ–≤–∞–µ—Ç –æ—Ç–Ω–æ—à–µ–Ω–∏–µ –∫ —Ç–µ–∫—Å—Ç—É –∫–∞–∫ –∫ –∞–≤—Ç–æ–Ω–æ–º–Ω–æ–π —Ä–µ–∞–ª—å–Ω–æ—Å—Ç–∏, –Ω–∞—Ü–µ–ª–µ–Ω–Ω–æ—Å—Ç—å –Ω–∞ –≤—ã—è–≤–ª–µ–Ω–∏–µ –µ–≥–æ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–π —Å—Ç—Ä—É–∫—Ç—É—Ä—ã.', '–†–µ–ø—Ä–µ–∑–µ–Ω—Ç–∞—Ç–∏–≤–Ω—ã–π ‚Äî —Ä–∞—Å—Å–º–æ—Ç—Ä–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∫–∞–∫ –æ—Å–æ–±–æ–π —Ñ–æ—Ä–º—ã –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –≤–Ω–µ—à–Ω–µ–π —Ç–µ–∫—Å—Ç—É –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏.']\n",
    "\n",
    "predicts = model.generate(encode(input_sequences)) \n",
    "\n",
    "decoded = tokenizer.batch_decode(predicts, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2301d2e-c706-4e20-9a83-36a3885b94c3",
   "metadata": {},
   "source": [
    "# –í–∞—Ä–∏–∞–Ω—Ç 2. –ü—Ä–æ—Å—Ç–æ –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "307ea349-4d90-43a5-8078-7b4b013e8363",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b7211c3f1d54545b6656282a2c5b6f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m model \u001b[38;5;241m=\u001b[39m T5ForConditionalGeneration\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRussianNLP/FRED-T5-Summarizer\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      5\u001b[0m device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 6\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m input_text\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<LM> –°–æ–∫—Ä–∞—Ç–∏ —Ç–µ–∫—Å—Ç.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m –¢–µ–∫—Å—Ç (–æ—Ç –ª–∞—Ç. textus ‚Äî —Ç–∫–∞–Ω—å; —Å–ø–ª–µ—Ç–µ–Ω–∏–µ, —Å–æ—á–µ—Ç–∞–Ω–∏–µ) ‚Äî –∑–∞—Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –Ω–∞ –∫–∞–∫–æ–º-–ª–∏–±–æ –º–∞—Ç–µ—Ä–∏–∞–ª—å–Ω–æ–º –Ω–æ—Å–∏—Ç–µ–ª–µ —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∞—è –º—ã—Å–ª—å; –≤ –æ–±—â–µ–º –ø–ª–∞–Ω–µ —Å–≤—è–∑–Ω–∞—è –∏ –ø–æ–ª–Ω–∞—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Å–∏–º–≤–æ–ª–æ–≤. –°—É—â–µ—Å—Ç–≤—É—é—Ç –¥–≤–µ –æ—Å–Ω–æ–≤–Ω—ã–µ —Ç—Ä–∞–∫—Ç–æ–≤–∫–∏ –ø–æ–Ω—è—Ç–∏—è ¬´—Ç–µ–∫—Å—Ç¬ª: –∏–º–º–∞–Ω–µ–Ω—Ç–Ω–∞—è (—Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è, —Ñ–∏–ª–æ—Å–æ—Ñ—Å–∫–∏ –Ω–∞–≥—Ä—É–∂–µ–Ω–Ω–∞—è) –∏ —Ä–µ–ø—Ä–µ–∑–µ–Ω—Ç–∞—Ç–∏–≤–Ω–∞—è (–±–æ–ª–µ–µ —á–∞—Å—Ç–Ω–∞—è). –ò–º–º–∞–Ω–µ–Ω—Ç–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –ø–æ–¥—Ä–∞–∑—É–º–µ–≤–∞–µ—Ç –æ—Ç–Ω–æ—à–µ–Ω–∏–µ –∫ —Ç–µ–∫—Å—Ç—É –∫–∞–∫ –∫ –∞–≤—Ç–æ–Ω–æ–º–Ω–æ–π —Ä–µ–∞–ª—å–Ω–æ—Å—Ç–∏, –Ω–∞—Ü–µ–ª–µ–Ω–Ω–æ—Å—Ç—å –Ω–∞ –≤—ã—è–≤–ª–µ–Ω–∏–µ –µ–≥–æ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–π —Å—Ç—Ä—É–∫—Ç—É—Ä—ã. –†–µ–ø—Ä–µ–∑–µ–Ω—Ç–∞—Ç–∏–≤–Ω—ã–π ‚Äî —Ä–∞—Å—Å–º–æ—Ç—Ä–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∫–∞–∫ –æ—Å–æ–±–æ–π —Ñ–æ—Ä–º—ã –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –≤–Ω–µ—à–Ω–µ–π —Ç–µ–∫—Å—Ç—É –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      9\u001b[0m input_ids\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mtensor([tokenizer\u001b[38;5;241m.\u001b[39mencode(input_text)])\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[1;32mC:\\Python311\\Lib\\site-packages\\transformers\\modeling_utils.py:2692\u001b[0m, in \u001b[0;36mPreTrainedModel.to\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2687\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_present_in_args:\n\u001b[0;32m   2688\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   2689\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou cannot cast a GPTQ model in a new `dtype`. Make sure to load the model using `from_pretrained` using the desired\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2690\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `dtype` by passing the correct `torch_dtype` argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2691\u001b[0m         )\n\u001b[1;32m-> 2692\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1173\u001b[0m, in \u001b[0;36mModule.to\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1170\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1171\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[1;32m-> 1173\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:779\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    777\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[0;32m    778\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 779\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    781\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    782\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    783\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    784\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    789\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    790\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:804\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    800\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[0;32m    801\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[0;32m    802\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[0;32m    803\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m--> 804\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    805\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[0;32m    807\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1159\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m   1152\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[0;32m   1153\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[0;32m   1154\u001b[0m             device,\n\u001b[0;32m   1155\u001b[0m             dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1156\u001b[0m             non_blocking,\n\u001b[0;32m   1157\u001b[0m             memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format,\n\u001b[0;32m   1158\u001b[0m         )\n\u001b[1;32m-> 1159\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1161\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1162\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1163\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1164\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1165\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot copy out of meta tensor; no data!\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32mC:\\Python311\\Lib\\site-packages\\torch\\cuda\\__init__.py:284\u001b[0m, in \u001b[0;36m_lazy_init\u001b[1;34m()\u001b[0m\n\u001b[0;32m    279\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    280\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    281\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultiprocessing, you must use the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspawn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m start method\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    282\u001b[0m     )\n\u001b[0;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(torch\u001b[38;5;241m.\u001b[39m_C, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_cuda_getDeviceCount\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 284\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTorch not compiled with CUDA enabled\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    285\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _cudart \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    286\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[0;32m    287\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    288\u001b[0m     )\n",
      "\u001b[1;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, T5ForConditionalGeneration \n",
    "tokenizer = GPT2Tokenizer.from_pretrained('RussianNLP/FRED-T5-Summarizer',eos_token='</s>')\n",
    "model = T5ForConditionalGeneration.from_pretrained('RussianNLP/FRED-T5-Summarizer')\n",
    "device='cuda'\n",
    "model.to(device)\n",
    "\n",
    "input_text='<LM> –°–æ–∫—Ä–∞—Ç–∏ —Ç–µ–∫—Å—Ç.\\n –¢–µ–∫—Å—Ç (–æ—Ç –ª–∞—Ç. textus ‚Äî —Ç–∫–∞–Ω—å; —Å–ø–ª–µ—Ç–µ–Ω–∏–µ, —Å–æ—á–µ—Ç–∞–Ω–∏–µ) ‚Äî –∑–∞—Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –Ω–∞ –∫–∞–∫–æ–º-–ª–∏–±–æ –º–∞—Ç–µ—Ä–∏–∞–ª—å–Ω–æ–º –Ω–æ—Å–∏—Ç–µ–ª–µ —á–µ–ª–æ–≤–µ—á–µ—Å–∫–∞—è –º—ã—Å–ª—å; –≤ –æ–±—â–µ–º –ø–ª–∞–Ω–µ —Å–≤—è–∑–Ω–∞—è –∏ –ø–æ–ª–Ω–∞—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Å–∏–º–≤–æ–ª–æ–≤. –°—É—â–µ—Å—Ç–≤—É—é—Ç –¥–≤–µ –æ—Å–Ω–æ–≤–Ω—ã–µ —Ç—Ä–∞–∫—Ç–æ–≤–∫–∏ –ø–æ–Ω—è—Ç–∏—è ¬´—Ç–µ–∫—Å—Ç¬ª: –∏–º–º–∞–Ω–µ–Ω—Ç–Ω–∞—è (—Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è, —Ñ–∏–ª–æ—Å–æ—Ñ—Å–∫–∏ –Ω–∞–≥—Ä—É–∂–µ–Ω–Ω–∞—è) –∏ —Ä–µ–ø—Ä–µ–∑–µ–Ω—Ç–∞—Ç–∏–≤–Ω–∞—è (–±–æ–ª–µ–µ —á–∞—Å—Ç–Ω–∞—è). –ò–º–º–∞–Ω–µ–Ω—Ç–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –ø–æ–¥—Ä–∞–∑—É–º–µ–≤–∞–µ—Ç –æ—Ç–Ω–æ—à–µ–Ω–∏–µ –∫ —Ç–µ–∫—Å—Ç—É –∫–∞–∫ –∫ –∞–≤—Ç–æ–Ω–æ–º–Ω–æ–π —Ä–µ–∞–ª—å–Ω–æ—Å—Ç–∏, –Ω–∞—Ü–µ–ª–µ–Ω–Ω–æ—Å—Ç—å –Ω–∞ –≤—ã—è–≤–ª–µ–Ω–∏–µ –µ–≥–æ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–π —Å—Ç—Ä—É–∫—Ç—É—Ä—ã. –†–µ–ø—Ä–µ–∑–µ–Ω—Ç–∞—Ç–∏–≤–Ω—ã–π ‚Äî —Ä–∞—Å—Å–º–æ—Ç—Ä–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –∫–∞–∫ –æ—Å–æ–±–æ–π —Ñ–æ—Ä–º—ã –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –≤–Ω–µ—à–Ω–µ–π —Ç–µ–∫—Å—Ç—É –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏.'\n",
    "input_ids=torch.tensor([tokenizer.encode(input_text)]).to(device)\n",
    "outputs=model.generate(input_ids,eos_token_id=tokenizer.eos_token_id,\n",
    "                    num_beams=5,\n",
    "                    min_new_tokens=17,\n",
    "                    max_new_tokens=200,\n",
    "                    do_sample=True,\n",
    "                    no_repeat_ngram_size=4,\n",
    "                    top_p=0.9)\n",
    "print(tokenizer.decode(outputs[0][1:]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
